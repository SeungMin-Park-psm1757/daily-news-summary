# ë¶í•œ êµ°ì‚¬/ì •ì¹˜ ë‰´ìŠ¤ í•„í„°ë§ + ìš”ì•½ í¬ë§· ê°œì„  (í—¤ë“œë¼ì¸ 3ê°œ, ì¤‘ë³µ ì œê±°, ìƒì„¸ 3ì¤„ ìš”ì•½)

def is_north_korea_military_or_politics(article):
    """ë¶í•œ êµ°ì‚¬/ì •ì¹˜ ë‰´ìŠ¤ í•„í„°ë§ í•¨ìˆ˜"""
    keywords = ['ë¶í•œ', 'ì¡°ì„ ', 'êµ°', 'êµ°ì‚¬', 'ì •ì¹˜', 'ê¹€ì •ì€', 'í•µ', 'ë¯¸ì‚¬ì¼']
    title = article.get('title', '')
    summary = article.get('summary', '')
    return any(k in title or k in summary for k in keywords)

def deduplicate_articles(articles, max_count=3):
    """ìœ ì‚¬ ê¸°ì‚¬ ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ ê°œìˆ˜ ì œí•œ"""
    seen = set()
    unique_articles = []
    for article in articles:
        key = article['title'].strip()
        if key not in seen and len(unique_articles) < max_count:
            seen.add(key)
            unique_articles.append(article)
    return unique_articles

async def collect_news_by_keyword(keyword, max_domestic=5, max_international=2):
    print(f"\nğŸ“° [{keyword}] ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    feeds = KEYWORD_FEEDS.get(keyword, [])
    domestic_articles = []
    international_articles = []

    async with aiohttp.ClientSession() as session:
        tasks = [fetch_feed(session, url) for url in feeds]
        feed_results = await asyncio.gather(*tasks)

    for feed_url, feed in zip(feeds, feed_results):
        if not feed or (feed.bozo and feed.bozo_exception):
            continue

        is_international = any(domain in feed_url for domain in ['nytimes.com', 'bbci.co.uk'])
        
        for entry in feed.entries[:10]:
            if not is_recent_article(entry.get('published')):
                continue

            # ë¶í•œ êµ°ì‚¬/ì •ì¹˜ í‚¤ì›Œë“œ í•„í„°ë§ ì ìš©
            if keyword in ['êµ°ëŒ€', 'ì •ì¹˜'] and not is_north_korea_military_or_politics(entry):
                continue

            title = clean_text(entry.get('title', ''))
            if not title or len(title) < 10:
                continue

            summary = clean_text(entry.get('summary', entry.get('description', '')))
            article = {
                'title': title[:100],
                'link': entry.get('link', ''),
                'summary': summary,
                'published': entry.get('published', ''),
                'source': 'international' if is_international else 'domestic'
            }

            if is_international and len(international_articles) < max_international:
                international_articles.append(article)
            elif not is_international and len(domestic_articles) < max_domestic:
                domestic_articles.append(article)

            if len(domestic_articles) >= max_domestic and len(international_articles) >= max_international:
                break
        if len(domestic_articles) >= max_domestic and len(international_articles) >= max_international:
            break

    # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 3ê°œë¡œ ì œí•œ (êµ­ë‚´+êµ­ì œ í•©ì‚°)
    all_articles = deduplicate_articles(domestic_articles + international_articles, max_count=3)
    return all_articles

def summarize_news_with_gemini(keyword, articles):
    if not articles:
        emoji = KEYWORD_EMOJIS.get(keyword, 'ğŸ“°')
        return f"{emoji} {keyword}\nâ€¢ ì˜¤ëŠ˜ì€ ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ê°€ ì—†ì—ˆìŠµë‹ˆë‹¤."

    # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 3ê°œ í—¤ë“œë¼ì¸
    unique_articles = deduplicate_articles(articles, max_count=3)
    headlines = []
    articles_text_for_summary = ""
    for i, article in enumerate(unique_articles, 1):
        headlines.append(f"{i}. {article['title']}")
        source_type = "ğŸŒí•´ì™¸" if article['source'] == 'international' else "ğŸ‡°ğŸ‡·êµ­ë‚´"
        articles_text_for_summary += f"\n[{source_type} ê¸°ì‚¬ {i}]\nì œëª©: {article['title']}\në‚´ìš©: {article['summary'][:300]}\n"

    headlines_text = "\n".join(headlines)
    emoji = KEYWORD_EMOJIS.get(keyword, 'ğŸ“°')

    # Gemini í”„ë¡¬í”„íŠ¸: êµ°ì‚¬/ì •ì¹˜ í•µì‹¬ ì´ìŠˆë§Œ, ë¶ˆí•„ìš” ì •ë³´ ì œì™¸, 3ì¤„ë¡œ ìƒì„¸ ìš”ì•½
    prompt = (
        f"'{keyword}' ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ ì¤‘ êµ°ì‚¬/ì •ì¹˜ ì´ìŠˆë§Œ ê³¨ë¼, ìœ ì‚¬í•œ ë‚´ìš©ì€ ìƒëµí•˜ê³ , 3ê°œì˜ ì£¼ìš” í—¤ë“œë¼ì¸ë§Œ ì œì‹œí•´ì¤˜. "
        "ê° ë‰´ìŠ¤ ì œëª©ì„ ë¨¼ì € ë‚˜ì—´í•˜ê³ , ì•„ë˜ì— í•µì‹¬ë§Œ 3ì¤„ë¡œ ìƒì„¸í•˜ê²Œ ìš”ì•½í•´ì¤˜. ë‚ ì”¨, ë¬¸í™”, ê¸°íƒ€ ë¹„ê´€ë ¨ ë‚´ìš©ì€ ëª¨ë‘ ì œì™¸í•´ì¤˜.\n"
        f"[ì£¼ìš” í—¤ë“œë¼ì¸]\n{headlines_text}\n\n[ì°¸ê³ ìš© ë‰´ìŠ¤ ë‚´ìš©]\n{articles_text_for_summary}\n[ìš”ì•½(3ì¤„)]"
    )

    try:
        response = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.5,
                max_output_tokens=1000,
                top_p=0.8,
                top_k=40
            )
        )
        summary = response.text.strip()
        if not summary:
            summary = "AI ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        formatted_summary = (
            f"{emoji} {keyword}\n"
            f"[ì£¼ìš” í—¤ë“œë¼ì¸]\n{headlines_text}\n\n"
            f"[ìš”ì•½]\n{summary.replace('*', '').strip()}"
        )
        return formatted_summary
    except Exception as e:
        error_summary = (
            f"{emoji} {keyword}\n"
            f"[ì£¼ìš” í—¤ë“œë¼ì¸]\n{headlines_text}\n\n"
            f"[ìš”ì•½]\n* AI ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. í—¤ë“œë¼ì¸ë§Œ ì°¸ê³ í•´ì£¼ì„¸ìš”."
        )
        return error_summary
